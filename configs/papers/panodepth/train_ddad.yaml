### 22.07.20. blooming-pine-41
# panodepth_ddad_resnet (resnet18)
# min/max depth: [2.0, 200.0]
# networks.depth.decoder.downsample: False
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False
# batch_size: 1 (single GPU)
# -> Best performance in overfitting (Epoch 22, 0.232 / 6.827 / 14.214 / 0.370 / 36.653 / 0.783 / 0.895 / 0.933)

        ### 22.07.20. fiery-morning-43
        # panodepth_ddad_resnet (resnet18)
        # min/max depth: [2.0, 200.0]
        # networks.depth.decoder.downsample: False
        # networks.depth.decoder.out_shape: [128, 1024]   [!]
        # losses.reprojection.flow_downsampling: False
        # losses.reprojection.upsample_depth: True        [!]
        # batch_size: 2 (4 GPUs, repeat: 8)               [!] (GPU Mem: 11GB)
        # -> Looks good in overfitting (Epoch 6, 0.328 / 9.021 / 17.306 / 0.458 / 45.210 / 0.642 / 0.810 / 0.881)

        ### 22.07.20. vague-oath-46
        # panodepth_ddad_resnet (resnet18)
        # min/max depth: [1.0, 200.0]                     [!]
        # networks.depth.scale_invdepth: inverse
        # networks.depth.decoder.downsample: False
        # networks.depth.decoder.out_shape: [128, 1024]
        # losses.reprojection.flow_downsampling: False
        # losses.reprojection.upsample_depth: True
        # batch_size: 4 (4 GPUs, repeat: 16)               [!] (GPU Mem: 18GB)
        # -> Not very good in overfitting (Epoch 25, 0.298 / 9.135 / 16.453 / 0.438 / 43.320 / 0.694 / 0.841 / 0.900)

        # => decoder.out_shape: [128, 1024] might seem to be problematic. (intrinsics-related?)
        # => Need to visualize features to see if the depth_sweeping works as intended or not.

### 22.07.20. dainty-moon-59
# panodepth_ddad_resnet (resnet18)
# min/max depth: [1.0, 200.0]                         [!]
# networks.depth.scale_invdepth: inverse
# networks.depth.decoder.downsample: False
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: True            [!]
# batch_size: 1 (single GPU, repeat: 1)
# -> Good performance in overfitting (Epoch 35, 0.244 / 7.538 / 13.995 / 0.373 / 37.212 / 0.790 / 0.895 / 0.935)

### 22.07.20. easy-sea-60 (Reproduce blooiming-pine-41)
# panodepth_ddad_resnet (resnet18)
# min/max depth: [2.0, 200.0]
# networks.depth.scale_invdepth: inverse
# networks.depth.decoder.downsample: False
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False
# batch_size: 1 (single GPU, repeat: 1)
# -> Successfully reproduced (Epoch 22, 0.234 / 6.496 / 14.190 / 0.372 / 36.762 / 0.774 / 0.892 / 0.933)

### 22.07.21. sunny-jazz-61
# panodepth_ddad_resnet (resnet18)
# min/max depth: [2.0, 200.0]
# networks.depth.scale_invdepth: inverse
# networks.depth.decoder.downsample: False
# networks.depth.decoder.out_shape: [128, 1024]       [!]   <- Need to fix!!
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False
# batch_size: 1 (single GPU, repeat: 1)
# ->

### 22.07.21. decent-breeze-62 (start working well! but why training is so slow? 2.5s/im)
# panodepth_ddad_resnet (resnet18)
# min/max depth: [1.0, 200.0]                         [!]
# networks.depth.scale_invdepth: inverse
# networks.depth.decoder.downsample: False
# networks.depth.decoder.out_shape: [128, 1024]       [!]
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False
# batch_size: 1 (single GPU, repeat: 1)
# -> Not bad (Epoch 17, 0.277 / 7.152 / 16.921 / 0.428 / 40.802 / 0.659 / 0.837 / 0.902)

### 22.07.21. super-vortex-63
#   [!!] Fix MultiDepthSweepFunction:
#     previous: given depths -> conv -> camera
#     updated: given depths + camera -> avg -> conv (to get consistent features)
# panodepth_ddad_resnet (resnet18)
# min/max depth: [1.0, 200.0]
# networks.depth.scale_invdepth: inverse
# networks.depth.decoder.downsample: False
# networks.depth.decoder.out_shape: [128, 1024]
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: True            [!]
# batch_size: 1 (single GPU, repeat: 1)
# -> Does't work at all. Maybe due to "upsample_depth: True"?


### 22.07.21. resilient-firebrand-65
#   [!!] Fix MultiDepthSweepFunction:
#     previous: given depths -> conv -> camera
#     updated: given depths + camera -> avg -> conv (to get consistent features)
#   + BasicBlock (ResNet block)                       [!]       (Next exp: what if we use Bottleneck instead of BasicBlock?)
# panodepth_ddad_resnet (resnet18)
# min/max depth: [1.0, 200.0]
#                                                               (Next exp: shared encoder)
#                                                               (Next exp: attention)
#                                                               (Next exp: positional encoding)
# networks.depth.scale_invdepth: inverse
# networks.depth.decoder.downsample: False
# networks.depth.decoder.out_shape: [128, 1024]
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False           [!]
# batch_size: 1 (single GPU, repeat: 1)               (GPU Mem: 6.8GB)
# -> Looks very good!!!! (Epoch 11, 0.265 / 6.799 / 16.219 / 0.398 / 39.399 / 0.702 / 0.854 / 0.917)


### 22.07.21. drawn-feather-69 (Slightly deeper pano-encoder)
#   + BasicBlock x 2 (ResNet block)                   [!]
# panodepth_ddad_resnet (resnet18)
# min/max depth: [1.0, 200.0]
#                                                               (Next exp: shared encoder)
#                                                               (Next exp: attention)
#                                                               (Next exp: positional encoding)
# networks.depth.scale_invdepth: inverse
# networks.depth.decoder.downsample: False
# networks.depth.decoder.out_shape: [128, 1024]
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False
# batch_size: 1 (single GPU, repeat: 1)               (GPU Mem: GB)
# -> Worse than resilient-firebrand-65, try to reproduce resilient-firebrand-65


### 22.07.21. whole-planet-70 (reproduce resilient-firebrand-65)
#   + BasicBlock (ResNet block)                   [!]
# panodepth_ddad_resnet (resnet18)
# min/max depth: [1.0, 200.0]
#                                                               (Next exp: shared encoder)
#                                                               (Next exp: attention)
#                                                               (Next exp: positional encoding)
# networks.depth.scale_invdepth: inverse
# networks.depth.decoder.downsample: False
# networks.depth.decoder.out_shape: [128, 1024]
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False
# batch_size: 1 (single GPU, repeat: 1)               (GPU Mem: 7.1GB)
# -> Not all scales are properly learned, but performances are okay
# -> (Epoch 20, 0.256 / 6.903 / 15.288 / 0.386 / 38.123 / 0.728 / 0.871 / 0.924)


### 22.07.22. restful-cherry-73 (Apply attention)
#   + BasicBlock (ResNet block)
#   + View-attention                              [!]
# panodepth_ddad_resnet (resnet18)
# min/max depth: [1.0, 200.0]
#                                                               (Next exp: shared encoder)
#                                                               (Next exp: positional encoding)
# networks.depth.scale_invdepth: inverse
# networks.depth.decoder.downsample: False
# networks.depth.decoder.out_shape: [128, 1024]
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False
# batch_size: 1 (single GPU, repeat: 1)               (GPU Mem: 7.7GB)
# -> Not all scales are properly learned, but performances are okay
# -> (Epoch 10, 0.272 / 6.715 / 16.578 / 0.413 / 38.497 / 0.650 / 0.841 / 0.911)

# NOTE: It seems that "making all scales work well" is a key to train a best model.
# In "resilient-firebrand-65", all scales worked, but how?


### 22.07.22. treasured-river-74 (Allow more depth hypotheses to cover very close objects)
#   + BasicBlock (ResNet block)
#   + View-attention                              [!]
# panodepth_ddad_resnet (resnet18)
# min/max depth: [1.0, 200.0]
#                                                               (Next exp: shared encoder)
#                                                               (Next exp: positional encoding)
# networks.depth.scale_invdepth: inverse
# networks.depth.decoder.downsample: False
# networks.depth.decoder.out_shape: [128, 1024]
# networks.depth.decoder.depth_hypothesis: [1, 3, 10, 30, 60, 90]   [!]
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False
# batch_size: 1 (single GPU, repeat: 1)               (GPU Mem: 8.6GB)
# -> Doesn't look good. But worth to try again later.


### 22.07.22. leafy-night-77 (Different depth hypotheses)
#   + BasicBlock (ResNet block)
#   + View-attention                              [!]
# panodepth_ddad_resnet (resnet18)
# min/max depth: [1.0, 200.0]
#                                                               (Next exp: shared encoder)
#                                                               (Next exp: positional encoding)
# networks.depth.scale_invdepth: inverse
# networks.depth.decoder.downsample: False
# networks.depth.decoder.out_shape: [128, 1024]
# networks.depth.decoder.depth_hypothesis: [1, 3, 10, 30, 90]   [!]
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False
# batch_size: 1 (single GPU, repeat: 1)               (GPU Mem: 8.3GB)
# -> Doesn't look good. But worth to try again later.


### 22.07.22. glad-aardvark-78 (Turn off view attention)
#   + BasicBlock (ResNet block)
#   + View-attention                              [!]
# panodepth_ddad_resnet (resnet18)
# min/max depth: [1.0, 200.0]
#                                                               (Next exp: shared encoder)
#                                                               (Next exp: positional encoding)
# networks.depth.scale_invdepth: inverse
# networks.depth.decoder.downsample: False
# networks.depth.decoder.out_shape: [128, 1024]
# networks.depth.decoder.depth_hypothesis: [1, 3, 10, 30, 90]
# networks.depth.decoder.view_attention: False                  [!]
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False
# batch_size: 1 (single GPU, repeat: 1)               (GPU Mem: 7.2GB)
# -> Two scales are trained, but not very good. Maybe depth_hypothesis is the reason (1m)


### 22.07.22. fancy-capybara-80 (Shared encoder: resnet34 / Previous depth hypotheses)
#                                                               (Next exp: positional encoding)
#                                                               (Next exp: scale weighting -> gamma: 0.8)
# networks.depth.recipe: networks/multicam_depth_net|panodepth_ddad_resnet_shared
# networks.depth.scale_invdepth: inverse
# networks.depth.decoder.downsample: False
# networks.depth.decoder.out_shape: [128, 1024]
# networks.depth.decoder.depth_hypothesis: [3, 10, 30, 90]
# networks.depth.decoder.view_attention: False
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False
# batch_size: 1 (single GPU, repeat: 1)               (GPU Mem: 5.9GB)
# -> Three scales! but hmm....

### 22.07.22. ruby-dragon-81 (depth_hypotheses: [2, ...] / min_depth: 2.0 / gamma: 0.8)
#                                                               (Next exp: positional encoding)
# networks.depth.recipe: networks/multicam_depth_net|panodepth_ddad_resnet
# networks.depth.scale_invdepth: inverse
# networks.depth.min_depth: 2.0                                   [!]
# networks.depth.decoder.downsample: False
# networks.depth.decoder.out_shape: [128, 1024]
# networks.depth.decoder.depth_hypothesis: [2, 3, 10, 30, 90]     [!]
# networks.depth.decoder.view_attention: False
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False
# losses.reprojection.gamma: 0.8                                  [!]
# batch_size: 1 (single GPU, repeat: 1)               (GPU Mem: 7.2GB)
# -> Two scales! (Epoch 4, 0.341 / 8.454 / 17.329 / 0.467 / 45.540 / 0.566 / 0.781 / 0.875)


### 22.07.22.
# networks.depth.recipe: networks/multicam_depth_net|panodepth_ddad_resnet
# networks.depth.scale_invdepth: inverse
# networks.depth.min_depth: 2.0                                   [!]
# networks.depth.decoder.downsample: False
# networks.depth.decoder.out_shape: [128, 1024]
# networks.depth.decoder.depth_hypothesis: [3, 10, 30, 90]
# networks.depth.decoder.view_attention: False
# networks.depth.decoder.positional_encoding: 32                  [!]
# losses.reprojection.flow_downsampling: False
# losses.reprojection.upsample_depth: False
# batch_size: 1 (single GPU, repeat: 1)               (GPU Mem: GB)
# ->

wrapper:
    recipe: wrapper|default
    flip_lr_prob: 0.0
    validate_flipped: False
    validate_first: False
    max_epochs: 35
arch:
    model:
        file: depth/PanoCamSelfSupGTPoseModel
    networks:
        depth:
            # recipe: networks/multicam_depth_net|panodepth_ddad
            recipe: networks/multicam_depth_net|panodepth_ddad_resnet
            # recipe: networks/multicam_depth_net|panodepth_ddad_resnet_shared      # (TODO) Need to fix!!!
            min_depth: 2.0
            max_depth: 200.0
            scale_invdepth: inverse
            # Note: downsampled panodepth can reduce computation and memory,
            # but it makes the image depth blurry when filling the holes by interpolation.
            decoder:
              out_shape: [128, 1024]
              depth_hypothesis: [3, 10, 30, 90]
              view_attention: False
              # # Control output scale further, since intrinsics are calculated from panocam definition (256 x 2048)
              # [IMPORTANT] DO NOT TURN THIS ON. We'll lose the sharpness of the depth map we get.
              downsample: False
              # depth_focal: 1.0
              positional_encoding: 32
    losses:
        reprojection:
            # recipe: losses/reprojection|panodepth_mono_only
            recipe: losses/reprojection|panodepth_mono_stereo
            automask_loss: True
            # Note: flow_downsampling can help mitigate holes in image depth issue,
            # but it makes the image depth blurry, so the smoothness calculated on image space
            # cannot make panodepth smooth.
            # Since current issue is "non-smooth panodepth" and "wrong depth" in very close region,
            # let's turn this off at this moment.
            flow_downsampling: False
            upsample_depth: False
            gamma: 1.0      # turn-off scale weighting
        smoothness:
            recipe: losses/smoothness|default
evaluation:
    panodepth:
        recipe: evaluation/depth|ddad_resize
        post_process: False
optimizers:
    depth:
        recipe: optimizers|adam_20_05
datasets:
    train:
        # recipe: datasets/ddad|train_selfsup_panodepth
        # path: [/data/datasets/DDAD/ddad_train_val/ddad_filtered.json]
        recipe: datasets/ddad_tiny|train_selfsup_panodepth
        labels: [depth, pose]
        repeat: [1]
        dataloader:
          # batch_size: 2
          batch_size: 1
          # num_workers: 24   # If pin_memory is True, requires ~59GB RAM per 8 workers
          num_workers: 32   # If pin_memory is True, requires ~59GB RAM per 8 workers
    validation:
        # recipe: datasets/ddad|validation_panodepth
        recipe: datasets/ddad_tiny|validation_panodepth
        labels: [depth, pose]
        dataloader:
          batch_size: 1
          num_workers: 16
wandb:
    recipe: wandb|default
    project: vidar_panodepth_lambda_overfitting
    tags: ['resnet18_encoders', 'train_on_filtered_DDAD']
    num_validation_logs: 10
checkpoint:
    recipe: checkpoint|default
