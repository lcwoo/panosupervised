{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e100ed82",
   "metadata": {},
   "source": [
    "## Jun 23, 2022\n",
    "\n",
    "# Generate dense PanoDepth GT\n",
    "- Load config: `configs/papers/panodepth/train_ddad.yaml`\n",
    "- Use PanoDepth GT\n",
    "- Visualize flows (from Pano to Camera, from Pano to Camera by flow reversal)\n",
    "- Visualize synthesized depth on camera from pano and compair it with depth on camera\n",
    "    - LiDAR -> Proj on PanoSpace -> Compute flows to Camera -> Flow reversal -> Grid sampling of panodepth using reversed flow\n",
    "    - LiDAR -> Proj on Camera\n",
    "- (Optional) Synthesize RGB on Camera using depth by reversed flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394fd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pythreejs as pjs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib.cm import get_cmap\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from IPython.core.display import display\n",
    "\n",
    "os.chdir('..')\n",
    "np.set_printoptions(precision=4)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf57d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from vidar.arch.losses.MultiCamPhotometricLoss import MultiCamPhotometricLoss\n",
    "from vidar.arch.losses.MultiViewPhotometricLoss import calc_smoothness\n",
    "from vidar.arch.networks.layers.panodepth.flow_reversal import FlowReversal\n",
    "from vidar.datasets.PanoCamOuroborosDataset import PANO_CAMERA_NAME\n",
    "from vidar.geometry.camera import Camera\n",
    "from vidar.geometry.camera_pano import PanoCamera\n",
    "from vidar.utils.config import cfg_has\n",
    "from vidar.utils.depth import inv2depth, depth2inv\n",
    "from vidar.utils.tensor import match_scales, make_same_resolution\n",
    "from vidar.utils.viz import viz_photo\n",
    "from vidar.utils.write import viz_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab7444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidar.utils.config import read_config\n",
    "\n",
    "config = read_config('configs/papers/panodepth/train_ddad.yaml')\n",
    "\n",
    "# Resize depth for easy debugging\n",
    "# config.datasets.train.augmentation.resize_supervision = True\n",
    "# config.datasets.train.dataloader.num_workers = 0\n",
    "config.datasets.validation.labels += ['lidar']\n",
    "config.datasets.validation.dataloader.batch_size = 1\n",
    "config.datasets.validation.dataloader.num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidar.utils.setup import setup_dataset, setup_dataloader\n",
    "\n",
    "# dataset = setup_dataset(config.datasets.train, verbose=True)\n",
    "# dataloader = setup_dataloader(dataset, config.datasets.train.dataloader, 'train')\n",
    "\n",
    "dataset = setup_dataset(config.datasets.validation, verbose=True)\n",
    "dataloader = setup_dataloader(dataset, config.datasets.train.dataloader, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9690adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_from_loader = next(iter(dataloader[0]))\n",
    "batch_from_loader.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5311df63",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "def to_uint8(array):\n",
    "    return (array * 255.0).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c951a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pythreejs as pjs\n",
    "\n",
    "def visualize_3d(xyz, rgb=None, size=0.03, height=480, width=480):\n",
    "    points_buf = pjs.BufferAttribute(array=xyz)\n",
    "    geometryAttrs = {'position': points_buf}\n",
    "\n",
    "    if rgb is not None:\n",
    "        colors_buf = pjs.BufferAttribute(array=rgb)\n",
    "        geometryAttrs['color'] = colors_buf\n",
    "    \n",
    "    geometry = pjs.BufferGeometry(attributes=geometryAttrs)\n",
    "\n",
    "    material = pjs.PointsMaterial(vertexColors='VertexColors', size=size)\n",
    "    pointCloud = pjs.Points(geometry=geometry, material=material)\n",
    "\n",
    "    pythreejs_camera = pjs.PerspectiveCamera(    \n",
    "        up=[1, 0, 1],\n",
    "        children=[pjs.DirectionalLight(color='white', intensity=0.5)])\n",
    "\n",
    "    pythreejs_camera.rotateX(np.pi/4)\n",
    "    pythreejs_camera.position = (-15., 0., 30.)\n",
    "\n",
    "    scene = pjs.Scene(children=[\n",
    "                    pointCloud,\n",
    "                    pythreejs_camera,\n",
    "                    pjs.AmbientLight(color='#777777')])\n",
    "    \n",
    "    axes = pjs.AxesHelper(size=3)\n",
    "    scene.add(axes)\n",
    "        \n",
    "    control = pjs.OrbitControls(controlling=pythreejs_camera)\n",
    "    renderer = pjs.Renderer(camera=pythreejs_camera, \n",
    "                        scene=scene, \n",
    "                        width=width,\n",
    "                        height=height,\n",
    "                        preserveDrawingBuffer=True,\n",
    "                        controls=[control])\n",
    "    \n",
    "    return renderer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36496cd9",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d88b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vidar.utils.config import load_class\n",
    "\n",
    "# depth_net = load_class('MultiCamDepthNet', 'vidar/arch/networks/depth')(config.arch.networks.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_batch['camera_pano']['intrinsics'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vidar.utils.types import is_dict\n",
    "\n",
    "# _input_keys = ('rgb', 'intrinsics', 'pose_to_pano')\n",
    "# filtered_batch = {}\n",
    "# t = 0\n",
    "# for cam, sample in batch_from_loader.items():\n",
    "#     if is_dict(sample):\n",
    "#         filtered_batch[cam] = {k: sample[k][t] if 'pano' not in cam else sample[k]\n",
    "#                                             for k in _input_keys if k in sample}\n",
    "\n",
    "# out = depth_net(filtered_batch)\n",
    "# out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidar.utils.config import load_class\n",
    "\n",
    "self = load_class('PanoDepthPhotometricLoss', 'vidar/arch/losses')(config.arch.losses.reprojection)\n",
    "self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "# from vidar.arch.blocks.depth.SigmoidToInvDepth import SigmoidToInvDepth\n",
    "\n",
    "# # min_depth, max_depth = config.arch.networks.depth.min_depth, config.arch.networks.depth.max_depth\n",
    "# min_depth, max_depth = (1.0, 200.0)\n",
    "# print(min_depth, max_depth)\n",
    "\n",
    "# scale_inv_depth = SigmoidToInvDepth(min_depth=min_depth, max_depth=max_depth)\n",
    "# init_out = torch.rand(batch_from_loader['camera_pano']['depth'].shape) * 0.0 + 0.5\n",
    "# inv_depth = scale_inv_depth(init_out)\n",
    "\n",
    "# print(inv2depth(inv_depth).min(), inv2depth(inv_depth).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidar.datasets.augmentations.resize import resize_torch_preserve\n",
    "\n",
    "return_logs = True\n",
    "\n",
    "pano_invdepths = [depth2inv(\n",
    "    resize_torch_preserve(batch_from_loader['camera_pano']['depth'], (128, 1024)))] * 4\n",
    "\n",
    "output = {'inv_depths': pano_invdepths}\n",
    "out = self(batch_from_loader, output, return_logs=return_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58701c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "out['log_images'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9efe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(out['log_images']['panodepth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2dc12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Flow reversal test\n",
    "camera_order = ['camera_07', 'camera_05', 'camera_01', 'camera_06', 'camera_08', 'camera_09']\n",
    "images = np.hstack([out['log_images']['warped_{}'.format(c)][::2, ::2] for c in camera_order])\n",
    "Image.fromarray(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8e190",
   "metadata": {},
   "source": [
    "# Effect of varying depth hypothesis\n",
    "## Prepare dense pano depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cdebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidar.arch.networks.layers.panodepth.depth_sweeping import FeatTransform\n",
    "\n",
    "decoder_required_keys = ('intrinsics', 'pose_to_pano')\n",
    "meta_info = {}\n",
    "t = 0       # Transforming features should be done in the same time frame.\n",
    "for cam, sample in batch_from_loader.items():\n",
    "    if not cam.startswith('camera'):\n",
    "        continue\n",
    "    meta_info[cam] = {k: sample[k][t] for k in decoder_required_keys if k in sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4773939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vidar.geometry.pose import Pose\n",
    "\n",
    "img_height, img_width = (384, 640)\n",
    "xyz_all, rgb_all = [], []\n",
    "for ii in tqdm(range(0, len(dataset[0]), 1)):\n",
    "    batch = dataset[0][ii]\n",
    "    xyz_lidar = batch['lidar_pointcloud'].astype(np.float32)\n",
    "#     extrinsics = batch['point_cloud']['extrinsics']\n",
    "#     xyz_ego = extrinsics[:3, :3] @ xyz_lidar.T + extrinsics[:3, 3:]\n",
    "    \n",
    "    pose = batch['lidar_pose'].astype(np.float32)\n",
    "    xyz_world = pose[:3, :3] @ xyz_lidar.T + pose[:3, 3:]\n",
    "    \n",
    "    # Get colors\n",
    "    rgb_world = np.zeros_like(xyz_world).T\n",
    "\n",
    "    for c in dataset[0].cameras:\n",
    "        camera = f'camera_0{c}'\n",
    "        # Tcw(camera_pose): Camera -> World pose transformation\n",
    "        K = batch[camera]['intrinsics'][0].numpy()\n",
    "#         Tcw = batch[camera]['extrinsics'][0]\n",
    "#         Twc = np.linalg.inv(Tcw)\n",
    "        Twc = batch[camera]['extrinsics'][0].numpy()\n",
    "        xyz_camera = Twc[:3, :3] @ xyz_lidar.T + Twc[:3, 3:]\n",
    "        ix, iy, iz = K @ xyz_camera\n",
    "        ix, iy = ((ix / iz).astype(np.int16), (iy / iz).astype(np.int16))\n",
    "\n",
    "        proj_on_image = np.logical_and.reduce([\n",
    "            xyz_camera[2] > 0,\n",
    "            ix >= 0, ix < img_width,\n",
    "            iy >= 0, iy < img_height,\n",
    "        ])\n",
    "\n",
    "        image = to_numpy(batch[camera]['rgb'][0])\n",
    "        rgb_world[proj_on_image] = image[iy[proj_on_image], ix[proj_on_image], :]\n",
    "        \n",
    "    xyz_all.append(xyz_world)\n",
    "    rgb_all.append(rgb_world)\n",
    "    \n",
    "xyz_all_world = np.hstack(xyz_all).T\n",
    "rgb_all = np.vstack(rgb_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dataset[0][0]\n",
    "\n",
    "pose = batch['lidar_pose'].astype(np.float32)\n",
    "pose_inv = np.linalg.inv(pose)\n",
    "xyz_all = pose_inv[:3, :3] @ xyz_all_world.T + pose_inv[:3, 3:]\n",
    "xyz_all = xyz_all.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_3d(xyz_all, rgb_all, size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399267e",
   "metadata": {},
   "source": [
    "## Synthesize PanoRGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(np.linalg.norm(xyz_all, 2, axis=1))[::-1]\n",
    "xyz_all = xyz_all[order]\n",
    "rgb_all = rgb_all[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dataset[0][0]\n",
    "\n",
    "K = torch.FloatTensor(batch['camera_pano']['intrinsics'])[None]\n",
    "Twc = torch.FloatTensor(batch['camera_pano']['Twc'])[None]\n",
    "hw = batch['camera_pano']['hw']\n",
    "xyz_all_tensor = torch.FloatTensor(xyz_all).T[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = PanoCamera(K, hw, Twc=Twc).project_points(xyz_all_tensor, from_world=True)\n",
    "\n",
    "coords[..., 0] = (coords[..., 0] + 1)/2 * hw[1]\n",
    "coords[..., 1] = (coords[..., 1] + 1)/2 * hw[0]\n",
    "coords = coords.long()\n",
    "mask =  (coords[..., 0] >= 0) & \\\n",
    "        (coords[..., 0] < hw[1]) & \\\n",
    "        (coords[..., 1] >= 0) & \\\n",
    "        (coords[..., 1] < hw[0])\n",
    "\n",
    "ix, iy = coords[mask].T\n",
    "\n",
    "pano_dense_depth = 0.5 * torch.ones([*hw, 3], dtype=torch.float32)\n",
    "pano_dense_depth[iy, ix, :] = torch.FloatTensor(rgb_all[mask.view(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4af7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(to_uint8(pano_dense_depth.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f6a8bf",
   "metadata": {},
   "source": [
    "## Draw depth-sweeping example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e43cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import cv2\n",
    "\n",
    "images = []\n",
    "pad = 10\n",
    "padding = 255 * np.ones((pad, 2048, 3), dtype=np.uint8)\n",
    "\n",
    "boxes = {}\n",
    "display(Image.fromarray(out['log_images']['panodepth'][:128]))\n",
    "images.append(out['log_images']['panodepth'][:128])\n",
    "images.append(padding)\n",
    "\n",
    "pano_dense_depth_rgb = to_uint8(pano_dense_depth.clone().numpy())\n",
    "display(Image.fromarray(pano_dense_depth_rgb))\n",
    "images.append(pano_dense_depth_rgb)\n",
    "images.append(padding)\n",
    "\n",
    "pano_dense_depth_box = copy.deepcopy(pano_dense_depth_rgb)\n",
    "\n",
    "boxes.update({\n",
    "    5: {\n",
    "        'start_point': (505, 120),\n",
    "        'end_point': (535, 150),\n",
    "        'color': [255, 0, 0],\n",
    "        'thickness': 3,\n",
    "    }\n",
    "})\n",
    "pano_dense_depth_box = cv2.rectangle(pano_dense_depth_box, \n",
    "                                     boxes[5]['start_point'], boxes[5]['end_point'], boxes[5]['color'], boxes[5]['thickness'])\n",
    "\n",
    "\n",
    "boxes.update({\n",
    "    30: {\n",
    "        'start_point': (760, 150),\n",
    "        'end_point': (810, 200),\n",
    "        'color': [217, 0, 255],\n",
    "        'thickness': 3,\n",
    "    }\n",
    "})\n",
    "\n",
    "pano_dense_depth_box = cv2.rectangle(pano_dense_depth_box, \n",
    "                                     boxes[30]['start_point'], boxes[30]['end_point'], boxes[30]['color'], boxes[30]['thickness'])\n",
    "\n",
    "\n",
    "\n",
    "boxes.update({\n",
    "    10: {\n",
    "        'start_point': (1190, 150),\n",
    "        'end_point': (1250, 210),\n",
    "        'color': [255, 192, 0],\n",
    "        'thickness': 3,\n",
    "    }\n",
    "})\n",
    "pano_dense_depth_box = cv2.rectangle(pano_dense_depth_box, \n",
    "                                     boxes[10]['start_point'], boxes[10]['end_point'], boxes[10]['color'], boxes[10]['thickness'])\n",
    "\n",
    "\n",
    "\n",
    "display(Image.fromarray(pano_dense_depth_box))\n",
    "images.append(pano_dense_depth_box)\n",
    "images.append(padding)\n",
    "\n",
    "# distances = [3, 5, 10, 30, 50, 90]\n",
    "distances = [3, 5, 10, 30]\n",
    "for d in distances:\n",
    "    transformed = []\n",
    "    for camera in ['camera_01', 'camera_05', 'camera_06', 'camera_07', 'camera_08', 'camera_09']:\n",
    "        module = FeatTransform(camera, 1.0, (3, 384, 640), (3, 256, 2048), given_depth=d)\n",
    "        transformed.append(module(batch_from_loader[camera]['rgb'][0], meta_info))\n",
    "    \n",
    "    num_views = torch.concat([t.sum(axis=1, keepdim=True) != 0.0 for t in transformed], axis=1)\n",
    "    num_views = num_views.sum(axis=1, keepdim=True).clamp(min=1.0)\n",
    "    transformed = torch.stack(transformed, axis=1).sum(axis=1) / num_views\n",
    "    transformed = to_uint8(to_numpy(transformed[0].detach()))\n",
    "    \n",
    "    \n",
    "    if d in boxes:\n",
    "        clr = boxes[d]['color']\n",
    "        thickness = 10\n",
    "        \n",
    "        transformed[ :thickness,   :] = clr\n",
    "        transformed[-thickness:,   :] = clr\n",
    "        transformed[  :,  :thickness] = clr\n",
    "        transformed[  :, -thickness:] = clr\n",
    "    \n",
    "    \n",
    "    for k, box in boxes.items():             \n",
    "        transformed = np.ascontiguousarray(transformed, dtype=np.uint8)\n",
    "#         thickness = 1 if k != d else box['thickness']\n",
    "        thickness = box['thickness']\n",
    "        transformed = cv2.rectangle(transformed, box['start_point'], box['end_point'], box['color'], thickness)\n",
    "\n",
    "    display(Image.fromarray(transformed))\n",
    "    images.append(transformed)\n",
    "    images.append(padding)\n",
    "    \n",
    "images = images[:-1]\n",
    "\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw with lines\n",
    "height, width = images[-1].shape[:2]\n",
    "\n",
    "images_resized = [cv2.resize(img, None, fx=width/img.shape[1], fy=width/img.shape[1]) for img in images]\n",
    "images_resized = np.vstack(images_resized)\n",
    "\n",
    "for d, box in boxes.items():\n",
    "    start = [int((box['start_point'][0] + box['end_point'][0])/2), int((pad + height) * 2 + box['start_point'][1])]\n",
    "    end = [int((box['start_point'][0] + box['end_point'][0])/2), int(images_resized.shape[0] - height + + box['end_point'][1])]\n",
    "    color = box['color']\n",
    "    thickness = 2\n",
    "    images_resized = cv2.line(images_resized, start, end, color, thickness)\n",
    "\n",
    "Image.fromarray(images_resized).save('motivation_multi_depth_sweeping_v2.png')\n",
    "Image.fromarray(images_resized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
