{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f387882c",
   "metadata": {},
   "source": [
    "## Jul 21, 2022\n",
    "\n",
    "# Simple modular debugger\n",
    "- Load config: `configs/papers/panodepth/train_ddad.yaml`\n",
    "- Create a module instance to debug\n",
    "- Run / visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a44620",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pythreejs as pjs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib.cm import get_cmap\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from IPython.core.display import display\n",
    "\n",
    "os.chdir('..')\n",
    "np.set_printoptions(precision=4)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from vidar.arch.losses.MultiCamPhotometricLoss import MultiCamPhotometricLoss\n",
    "from vidar.arch.losses.MultiViewPhotometricLoss import calc_smoothness\n",
    "from vidar.arch.networks.layers.panodepth.flow_reversal import FlowReversal\n",
    "from vidar.datasets.PanoCamOuroborosDataset import PANO_CAMERA_NAME\n",
    "from vidar.geometry.camera import Camera\n",
    "from vidar.geometry.camera_pano import PanoCamera\n",
    "from vidar.utils.config import cfg_has\n",
    "from vidar.utils.depth import inv2depth, depth2inv\n",
    "from vidar.utils.tensor import match_scales, make_same_resolution\n",
    "from vidar.utils.viz import viz_photo\n",
    "from vidar.utils.write import viz_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import to_numpy, to_uint8, visualize_3d, visualize_3d_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidar.utils.config import read_config\n",
    "\n",
    "config = read_config('configs/papers/panodepth/train_ddad.yaml')\n",
    "\n",
    "config.arch.networks.depth.decoder.out_shape = [128, 1024]\n",
    "# config.arch.networks.depth.decoder.out_shape = [64, 512]\n",
    "    \n",
    "# Resize depth for easy debugging\n",
    "# config.datasets.train.augmentation.resize_supervision = True\n",
    "# config.datasets.train.dataloader.num_workers = 0\n",
    "config.datasets.validation.labels += ['lidar']\n",
    "config.datasets.validation.dataloader.batch_size = 1\n",
    "config.datasets.validation.dataloader.num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.arch.losses.reprojection.reprojection_pairs = [\n",
    "#     ['camera_09', 0, 'camera_09', -1],\n",
    "#     ['camera_09', 0, 'camera_09', 1],\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dbb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidar.utils.setup import setup_dataset, setup_dataloader\n",
    "\n",
    "# dataset = setup_dataset(config.datasets.train, verbose=True)\n",
    "# dataloader = setup_dataloader(dataset, config.datasets.train.dataloader, 'train')\n",
    "\n",
    "dataset = setup_dataset(config.datasets.validation, verbose=True)\n",
    "# dataloader = setup_dataloader(dataset, config.datasets.validation.dataloader, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "batch_from_loader = default_collate([dataset[0][0]])\n",
    "batch_from_loader.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316911fe",
   "metadata": {},
   "source": [
    "## DEBUG FeatTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6035da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import cv2\n",
    "\n",
    "from vidar.arch.networks.layers.panodepth.depth_sweeping import FeatTransform\n",
    "\n",
    "pad = 10\n",
    "padding = 255 * np.ones((pad, 2048, 3), dtype=np.uint8)\n",
    "\n",
    "boxes = {}\n",
    "display(Image.fromarray(out['log_images']['panodepth'][:128]))\n",
    "images.append(out['log_images']['panodepth'][:128])\n",
    "images.append(padding)\n",
    "\n",
    "\n",
    "decoder_required_keys = ('intrinsics', 'pose_to_pano')\n",
    "meta_info = {}\n",
    "t = 0       # Transforming features should be done in the same time frame.\n",
    "for cam, sample in batch_from_loader.items():\n",
    "    if not cam.startswith('camera'):\n",
    "        continue\n",
    "    meta_info[cam] = {k: sample[k][t] for k in decoder_required_keys if k in sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a779a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_info['camera_pano']\n",
    "# config.arch.networks.depth.decoder.out_shape = [16, 128]\n",
    "# config.arch.networks.depth.decoder.out_shape = [32, 256]\n",
    "config.arch.networks.depth.decoder.out_shape = [128, 1024]\n",
    "oscale = config.arch.networks.depth.decoder.ref_shape[0] // config.arch.networks.depth.decoder.out_shape[0]\n",
    "oscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.arch.networks.depth.decoder.out_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c1e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances = [3, 5, 10, 30]\n",
    "out_shape = config.arch.networks.depth.decoder.out_shape\n",
    "\n",
    "distances = [10]\n",
    "for d in distances:\n",
    "    transformed = []\n",
    "    for camera in ['camera_01', 'camera_05', 'camera_06', 'camera_07', 'camera_08', 'camera_09']:\n",
    "#         module = FeatTransform(camera, 1.0, (3, 384, 640), 1.0*oscale, (3, 256, 2048), given_depth=d)\n",
    "#         module = FeatTransform(camera, 1.0, (3, 384, 640), 1.0*oscale, (3, 128, 1024), given_depth=d)\n",
    "        module = FeatTransform(camera, 1.0, (3, 384, 640), 1.0*oscale, [3] + out_shape, given_depth=d)\n",
    "        transformed.append(module(batch_from_loader[camera]['rgb'][0], meta_info))\n",
    "    \n",
    "    num_views = torch.concat([t.sum(axis=1, keepdim=True) != 0.0 for t in transformed], axis=1)\n",
    "    num_views = num_views.sum(axis=1, keepdim=True).clamp(min=1.0)\n",
    "#     transformed = torch.stack(transformed, axis=1).sum(axis=1) / num_views\n",
    "    transformed = torch.stack(transformed, axis=1).sum(axis=1)\n",
    "    transformed = to_uint8(to_numpy(transformed[0].detach()))\n",
    "    \n",
    "    display(Image.fromarray(transformed).resize((1024, 128)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db2a8b9",
   "metadata": {},
   "source": [
    "## DEBUG DepthNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270bfd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vidar.utils.config import read_config\n",
    "\n",
    "# config = read_config('configs/papers/panodepth/train_ddad.yaml')\n",
    "\n",
    "# # config.arch.networks.depth.decoder.out_shape = [256, 2048]\n",
    "# config.arch.networks.depth.decoder.out_shape = [128, 1024]\n",
    "# # config.arch.networks.depth.decoder.out_shape = [64, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b0a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidar.utils.config import load_class\n",
    "\n",
    "depth_net = load_class('MultiCamDepthNet', 'vidar/arch/networks/depth')(config.arch.networks.depth)\n",
    "depth_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05928d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidar.utils.types import is_dict\n",
    "\n",
    "_input_keys = ('rgb', 'intrinsics', 'pose_to_pano')\n",
    "return_logs = True\n",
    "\n",
    "ctx = 0\n",
    "filtered_batch = {}\n",
    "for cam, sample in batch_from_loader.items():\n",
    "    if is_dict(sample):\n",
    "        filtered_batch[cam] = {k: sample[k][ctx] for k in _input_keys if k in sample}\n",
    "\n",
    "net_output = depth_net(filtered_batch, return_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39364dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_order = ['camera_07', 'camera_05', 'camera_01', 'camera_06', 'camera_08', 'camera_09']\n",
    "images = np.hstack([to_uint8(to_numpy(batch_from_loader[c]['rgb'][0][0])) for c in camera_order])\n",
    "# batch_from_loader['camera_01']['rgb'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70430c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_output['log_images'].keys()\n",
    "display(Image.fromarray(images))\n",
    "display(Image.fromarray(net_output['log_images']['input_agg_feats']).resize((1024, 128*5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_output['log_images'].keys()\n",
    "display(Image.fromarray(images))\n",
    "display(Image.fromarray(net_output['log_images']['input_agg_feats']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74282f13",
   "metadata": {},
   "source": [
    "## DEBUG Loss module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb112bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidar.utils.config import load_class\n",
    "\n",
    "self = load_class('PanoDepthPhotometricLoss', 'vidar/arch/losses')(config.arch.losses.reprojection)\n",
    "self.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidar.datasets.augmentations.resize import resize_torch_preserve\n",
    "\n",
    "return_logs = True\n",
    "\n",
    "pano_invdepths = [depth2inv(\n",
    "    resize_torch_preserve(batch_from_loader['camera_pano']['depth'][0], (128, 1024)))] * 4\n",
    "\n",
    "output = {'inv_depths': pano_invdepths}\n",
    "out = self(batch_from_loader, output, return_logs=return_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vidar.arch.networks.layers.panodepth.depth_sweeping import FeatTransform\n",
    "\n",
    "# decoder_required_keys = ('intrinsics', 'pose_to_pano')\n",
    "# meta_info = {}\n",
    "# t = 0       # Transforming features should be done in the same time frame.\n",
    "# for cam, sample in batch_from_loader.items():\n",
    "#     if not cam.startswith('camera'):\n",
    "#         continue\n",
    "#     meta_info[cam] = {k: sample[k][t] for k in decoder_required_keys if k in sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9920aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances = [3, 5, 10, 30]\n",
    "# for d in distances:\n",
    "#     transformed = []\n",
    "#     for camera in ['camera_01', 'camera_05', 'camera_06', 'camera_07', 'camera_08', 'camera_09']:\n",
    "#         module = FeatTransform(camera, 1.0, (3, 384, 640), (3, 256, 2048), given_depth=d)\n",
    "#         transformed.append(module(batch_from_loader[camera]['rgb'][0], meta_info))\n",
    "    \n",
    "#     num_views = torch.concat([t.sum(axis=1, keepdim=True) != 0.0 for t in transformed], axis=1)\n",
    "#     num_views = num_views.sum(axis=1, keepdim=True).clamp(min=1.0)\n",
    "#     transformed = torch.stack(transformed, axis=1).sum(axis=1) / num_views\n",
    "#     transformed = to_uint8(to_numpy(transformed[0].detach()))\n",
    "        \n",
    "#     display(Image.fromarray(transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff7eb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image.fromarray(out['log_images']['panodepth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751bf0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Flow reversal by 4-points, (1m, 200m) inv_depth 0.5\n",
    "camera_order = ['camera_07', 'camera_05', 'camera_01', 'camera_06', 'camera_08', 'camera_09']\n",
    "images = np.hstack([out['log_images']['warped_{}'.format(c)][::2, ::2] for c in camera_order])\n",
    "Image.fromarray(images)\n",
    "# Image.fromarray(out['log_images']['warped_camera_01'][::2, ::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bffcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
