{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chungwoo/workspace/panodepth-vidar\n"
     ]
    }
   ],
   "source": [
    "%cd /home/chungwoo/workspace/panodepth-vidar/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from vidar.datasets.augmentations.resize import resize_npy_preserve\n",
    "from vidar.geometry.camera_pano import PanoCamera\n",
    "from vidar.datasets.OuroborosDataset import load_from_file, save_to_file, generate_proj_maps\n",
    "from vidar.datasets.OuroborosDataset import OuroborosDataset\n",
    "from vidar.datasets.utils.misc import stack_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANO_CAMERA_NAME = 'camera_pano'\n",
    "\n",
    "\n",
    "def generate_pano_proj_maps(camera, Xw, Xl):\n",
    "    \"\"\"Render pointcloud on pano image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    camera: PanoCamera\n",
    "        Camera object with appropriately set extrinsics wrt world.\n",
    "    Xw: np.Array\n",
    "        3D point cloud (x, y, z) in the world coordinate. [N,3]\n",
    "    Xl: np.Array\n",
    "        3D point cloud (x, y, z) in the lidar coordinate. [N,3]\n",
    "    Returns\n",
    "    -------\n",
    "    depth: np.Array\n",
    "        Rendered pano depth image\n",
    "    \"\"\"\n",
    "    # Project the points\n",
    "    uv_tensor, rho_tensor = camera.project_points(Xw, normalize=False, return_z=True)\n",
    "    uv = uv_tensor[0].numpy().astype(int)\n",
    "    # Colorize the point cloud based on depth\n",
    "    rho = rho_tensor[0].numpy()\n",
    "\n",
    "    # Create an empty image to overlay\n",
    "    H, W = camera.hw\n",
    "    proj_depth = np.zeros((H, W), dtype=np.float32)\n",
    "    in_view = np.logical_and.reduce([(uv >= 0).all(axis=1), uv[:, 0] < W, uv[:, 1] < H, rho > 0])\n",
    "    uv, rho = uv[in_view], rho[in_view]\n",
    "\n",
    "    # TODO(sohwang): this is not enough, we need meshes and filter points by z-buffer.\n",
    "    # Sort by distance to pick closest one if multiple LiDAR points are projected onto a single pixel\n",
    "    order = np.argsort(rho)[::-1]\n",
    "    uv = uv[order]\n",
    "    rho = rho[order]\n",
    "    proj_depth[uv[:, 1], uv[:, 0]] = rho\n",
    "\n",
    "    # Calculate yaw angle in LiDAR coordinate\n",
    "    xx = Xl[in_view][:, 0]\n",
    "    yy = Xl[in_view][:, 1]\n",
    "    yaw = np.arctan2(yy, xx + 1e-6)\n",
    "\n",
    "    # HACK(soonminh): Reverse yaw to make it clockwise and add pi to start from backward\n",
    "    yaw = -yaw + np.pi\n",
    "\n",
    "    proj_angle = np.zeros((H, W), dtype=np.float32)\n",
    "    proj_angle[uv[:, 1], uv[:, 0]] = yaw\n",
    "\n",
    "    return proj_depth, proj_angle\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "#### DATASET\n",
    "########################################################################################################################\n",
    "class MultiCamOuroborosDataset(OuroborosDataset):\n",
    "    \"\"\"\n",
    "    MultiCamOuroborosDataset dataset class for MultiCam models, which inherits OuroborosDataset.\n",
    "    This class returns per-camera dictionary of batches for MultiCam models\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs, do_stack_samples=False)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        samples, lidar_sample = super().__getitem__(idx)\n",
    "\n",
    "        samples_dict = {sample['sensor_name'].lower(): stack_sample([sample]) for sample in samples}\n",
    "        samples_dict['idx'] = samples[0]['idx']\n",
    "        samples_dict.update(lidar_sample)\n",
    "        return samples_dict\n",
    "\n",
    "\n",
    "class PanoCamOuroborosDataset(MultiCamOuroborosDataset):\n",
    "    \"\"\"\n",
    "    PanoCamOuroborosDataset dataset class for PanoCam models, which inherits MultiCamOuroborosDataset.\n",
    "    This class returns per-camera dictionary of batches for PanoCam models and PanoDepth GT for evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pano_cfg = kwargs.pop('pano_cam_config')\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.pano_name = pano_cfg.name\n",
    "        self.pano_cfg = pano_cfg.dict\n",
    "\n",
    "    def panodepth_to_points(self, distance):\n",
    "        \"\"\"\n",
    "        Unproject depth from a camera's perspective into a world-frame pointcloud\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        depth : np.Array\n",
    "            Depth map to be lifted [H,W]\n",
    "        datum_idx : Int\n",
    "            Index of the camera\n",
    "        coord: String (world, ego, cam)\n",
    "            Coordinate of the output points\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pointcloud : np.Array\n",
    "            Lifted 3D pointcloud [Nx3]\n",
    "        \"\"\"\n",
    "        params = PanoCamera.params_from_config(self.pano_cfg)\n",
    "        hw = params['hw']\n",
    "        K = params['K']\n",
    "\n",
    "        Twc = params['Twc']\n",
    "        h, w = hw\n",
    "        pcl = PanoCamera(K[None], hw, Twc=Twc[None]).reconstruct_depth_map(\n",
    "            distance * torch.ones([1, 1, h, w]), to_world=True)\n",
    "        return pcl.numpy().reshape(3, -1).T\n",
    "\n",
    "\n",
    "    def create_pano_rays(self, distance=1.0):\n",
    "        image_shape = PanoCamera.params_from_config(self.pano_cfg)['hw']\n",
    "        pcl = self.panodepth_to_points(distance)\n",
    "\n",
    "        ### Calculate polar/azimuth angles in LiDAR coordinate\n",
    "        # [LiDAR coordinate convention] https://en.wikipedia.org/wiki/Spherical_coordinate_system\n",
    "        #   (X, Y, Z) = (N, W, U), where northing (N), westing(W), and upwardness (U)\n",
    "        #   polar angle (theta): measured from a fixed zenith direction\n",
    "        #   azimuth angle (phi): measured from negative X-axis (southing, S) on a reference (XY-) plane\n",
    "        r = np.linalg.norm(pcl, 2, axis=1)\n",
    "        x, y, z = pcl.T\n",
    "\n",
    "        # measured from Z-axis\n",
    "        theta = np.arccos(z / (r + 1e-6))\n",
    "        # measured from X-axis, counterclockwise\n",
    "        phi = np.arctan2(y, x + 1e-6)\n",
    "        # Make phi positive/clockwise angle\n",
    "        phi = -phi + np.pi\n",
    "\n",
    "        rays = np.stack([theta, phi], axis=0).reshape(2, *image_shape)\n",
    "        return rays\n",
    "\n",
    "\n",
    "        ### Calculate polar/azimuth angles in LiDAR coordinate\n",
    "        # [LiDAR coordinate convention] https://en.wikipedia.org/wiki/Spherical_coordinate_system\n",
    "        #   (X, Y, Z) = (N, W, U), where northing (N), westing(W), and upwardness (U)\n",
    "        #   polar angle (theta): measured from a fixed zenith direction\n",
    "        #   azimuth angle (phi): measured from negative X-axis (southing, S) on a reference (XY-) plane\n",
    "\n",
    "        image_shape = (self.pano_cfg['height'], self.pano_cfg['width'])\n",
    "\n",
    "        # TODO(soonminh): change to spherical coordinate\n",
    "        rho = self.pano_cfg['rho']\n",
    "        theta_range = [np.arctan2(abs(z), rho) for z in self.pano_cfg['z_range']]\n",
    "        theta_range[0] = np.pi / 2 - theta_range[0]\n",
    "        theta_range[1] += np.pi / 2\n",
    "\n",
    "        theta = np.linspace(*theta_range, num=image_shape[0])\n",
    "        phi = np.linspace(0, 2 * np.pi + 1e-6, num=image_shape[1])\n",
    "\n",
    "        theta, phi = np.meshgrid(theta, phi, indexing='ij')\n",
    "        rays = np.stack([theta, phi], axis=0)\n",
    "        return rays\n",
    "\n",
    "    def create_pano_proj_maps(self, filename, K, hw, Twc, depth_idx, depth_type):\n",
    "        \"\"\"\n",
    "        Creates the depth map for a camera by projecting LiDAR information.\n",
    "        It also caches the depth map following DGP folder structure, so it's not recalculated\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : String\n",
    "            Filename used for loading / saving\n",
    "        depth_idx : Int\n",
    "            Depth sensor index\n",
    "        depth_type : String\n",
    "            Which depth type will be loaded\n",
    "        world_points : np.Array [Nx3]\n",
    "            Points that will be projected (optional)\n",
    "        context : Int\n",
    "            Context value for choosing current of reference information\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        depth : np.Array\n",
    "            Depth map for that datum in that sample [H,W]\n",
    "        \"\"\"\n",
    "        filename_depth = '{}/{}.npz'.format(\n",
    "            os.path.dirname(self.path), filename.format('depth/{}'.format(depth_type)))\n",
    "        # Load and return if exists\n",
    "        try:\n",
    "            # Get cached depth map\n",
    "            # depth, depth_yaw = load_from_file(filename_depth, 'depth', 'depth_yaw')\n",
    "            depth, angle = load_from_file(filename_depth, ['depth', 'angle'])\n",
    "            return depth, angle\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Get lidar information\n",
    "        lidar_extrinsics = self.get_current('extrinsics', depth_idx)\n",
    "        lidar_points = self.get_current('point_cloud', depth_idx)\n",
    "        world_points = (lidar_extrinsics * lidar_points).T\n",
    "\n",
    "        # Create camera\n",
    "        camera = PanoCamera(K[None], hw, Twc=Twc[None])\n",
    "        world_points = torch.FloatTensor(world_points[None])\n",
    "\n",
    "        # Generate depth maps\n",
    "        depth, angle = generate_pano_proj_maps(camera, world_points, lidar_points)\n",
    "\n",
    "        save_to_file(filename_depth, {'depth': depth, 'angle': angle})\n",
    "        return depth, angle\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        samples = super().__getitem__(idx)\n",
    "        filename_chunk = self.get_filename(idx, 0).split('/')\n",
    "        filename_chunk[-2] = os.path.join(PANO_CAMERA_NAME.upper(), self.pano_name)\n",
    "        filename = '/'.join(filename_chunk)\n",
    "\n",
    "        # lidar_pose = torch.FloatTensor(self.get_current('pose', self.depth_idx).matrix).inverse()\n",
    "\n",
    "        # Extrinsics: A pose of sensor wrt the body frame. (but maybe corrupted? not sure yet.)\n",
    "        # Pose: 4 x 4 transformation from sensor to world\n",
    "\n",
    "        # From dgp/datasets/base_dataset.py:L1381\n",
    "        # \"extrinsics\": Pose\n",
    "        #   Camera extrinsics with respect to the vehicle frame, if available.\n",
    "        # \"pose\": Pose\n",
    "        #   Pose of sensor with respect to the world/global/local frame\n",
    "        #   (reference frame that is initialized at start-time). (i.e. this\n",
    "        #   provides the ego-pose in `pose_WC`).\n",
    "\n",
    "        # TODO(soonminh): follow the same convention with OuroborosDataset\n",
    "        # e.g. some entities such as intrinsics and depth should be dicts by context index\n",
    "        params = PanoCamera.params_from_config(self.pano_cfg)\n",
    "        hw = params['hw']\n",
    "        K = params['K']\n",
    "        # Twc = params['Twc'] @ lidar_pose\n",
    "        # extrinsics = params['Twc']\n",
    "        # Twc = extrinsics @ lidar_pose\n",
    "        # Twc = extrinsics\n",
    "        Twc = params['Twc']\n",
    "        samples[PANO_CAMERA_NAME.lower()] = {\n",
    "            'filename': {0: filename},\n",
    "            'hw': hw,\n",
    "            'intrinsics': {0: K},\n",
    "            'Twc': Twc,\n",
    "            # 'extrinsics': {0: extrinsics}\n",
    "        }\n",
    "\n",
    "        # Rays\n",
    "        if self.with_rays:\n",
    "            rays = self.create_pano_rays()\n",
    "            embedding = np.stack([\n",
    "                np.sin(rays[0]),\n",
    "                np.cos(rays[0]),\n",
    "                np.sin(rays[1]),\n",
    "                np.cos(rays[1]),\n",
    "            ], axis=0)\n",
    "            samples[PANO_CAMERA_NAME.lower()].update({\n",
    "                'rays': {0: rays},\n",
    "                'rays_embedding': {0: embedding},\n",
    "            })\n",
    "\n",
    "        for i in range(self.num_cameras):\n",
    "            camera = self.get_current('datum_name', i).lower()\n",
    "            pose_to_pano = Twc @ torch.FloatTensor(samples[camera]['extrinsics'][0]).inverse()\n",
    "            # pose_to_pano = Twc @ torch.FloatTensor(samples[camera]['pose'][0]).inverse() # (camera -> world) -> (world -> pano) == (camera -> pano)\n",
    "            # pose_to_pano_orig = params['Twc'] @ torch.FloatTensor(samples[camera]['extrinsics'][0]).inverse()\n",
    "            samples[camera]['pose_to_pano'] = {0: pose_to_pano}\n",
    "\n",
    "        if self.with_depth:\n",
    "            depth, angle = self.create_pano_proj_maps(filename, K, hw, Twc, self.depth_idx, self.depth_type)\n",
    "            depth = resize_npy_preserve(depth, hw, expand_dims=False)\n",
    "            angle = resize_npy_preserve(angle, hw, expand_dims=False)\n",
    "            samples[PANO_CAMERA_NAME.lower()]['depth'] = {0: depth.astype(np.float32)[None]}\n",
    "            samples[PANO_CAMERA_NAME.lower()]['angle'] = {0: angle.astype(np.float32)[None]}\n",
    "\n",
    "        return samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chungwoo/workspace/panodepth-vidar/dgp/datasets/frame_dataset.py:20: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  np.zeros((len(DATUM_TYPE_TO_SUPPORTED_ANNOTATION_TYPE), len(ALL_ANNOTATION_TYPES)), dtype=np.bool),\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 34\u001b[0m\n\u001b[1;32m     11\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPanoCamOuroboros\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/datasets/DDAD/ddad_train_val/ddad_overfit_000071.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m         get_transforms(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, Config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresize\u001b[39m\u001b[38;5;124m'\u001b[39m: [height, width]})),\n\u001b[1;32m     31\u001b[0m }\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Initialize dataset\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPanoCamOuroborosDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m# of frames: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(dataset)))\n",
      "Cell \u001b[0;32mIn[7], line 80\u001b[0m, in \u001b[0;36mPanoCamOuroborosDataset.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     79\u001b[0m     pano_cfg \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpano_cam_config\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpano_name \u001b[38;5;241m=\u001b[39m pano_cfg\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpano_cfg \u001b[38;5;241m=\u001b[39m pano_cfg\u001b[38;5;241m.\u001b[39mdict\n",
      "Cell \u001b[0;32mIn[7], line 62\u001b[0m, in \u001b[0;36mMultiCamOuroborosDataset.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_stack_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/panodepth-vidar/vidar/datasets/OuroborosDataset.py:182\u001b[0m, in \u001b[0;36mOuroborosDataset.__init__\u001b[0;34m(self, split, tag, depth_type, input_depth_type, masks, do_stack_samples, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Choose which dataset to use\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvirtual:\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdgp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msynchronized_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SynchronizedSceneDataset\n\u001b[1;32m    183\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m SynchronizedSceneDataset\n\u001b[1;32m    184\u001b[0m     extra_args \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/workspace/panodepth-vidar/dgp/datasets/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2019-2021 Toyota Research Institute. All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdgp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseDataset, DatasetMetadata  \u001b[38;5;66;03m#isort:skip\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdgp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframe_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m#isort:skip\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     FrameScene, FrameSceneDataset\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdgp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msynchronized_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# isort:skip\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     SynchronizedScene, SynchronizedSceneDataset\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdgp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpd_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ParallelDomainScene, ParallelDomainSceneDataset)\n",
      "File \u001b[0;32m~/workspace/panodepth-vidar/dgp/datasets/frame_dataset.py:20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdgp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ALL_ANNOTATION_TYPES, DATUM_TYPE_TO_SUPPORTED_ANNOTATION_TYPE)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdgp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseDataset, DatasetMetadata\n\u001b[1;32m     19\u001b[0m SUPPORTED_ANNOTATIONS_TABLE \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mDataArray(\n\u001b[0;32m---> 20\u001b[0m     np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(DATUM_TYPE_TO_SUPPORTED_ANNOTATION_TYPE), \u001b[38;5;28mlen\u001b[39m(ALL_ANNOTATION_TYPES)), dtype\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m),\n\u001b[1;32m     21\u001b[0m     dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatum_types\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     22\u001b[0m     coords\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatum_types\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(DATUM_TYPE_TO_SUPPORTED_ANNOTATION_TYPE),\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(ALL_ANNOTATION_TYPES)\n\u001b[1;32m     25\u001b[0m     }\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m datum_type_, annotations_ \u001b[38;5;129;01min\u001b[39;00m DATUM_TYPE_TO_SUPPORTED_ANNOTATION_TYPE\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m annotation_ \u001b[38;5;129;01min\u001b[39;00m annotations_:\n",
      "File \u001b[0;32m~/anaconda3/envs/ddad/lib/python3.8/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from struct import pack, unpack\n",
    "\n",
    "from vidar.datasets.utils.transforms import get_transforms\n",
    "from vidar.utils.config import Config\n",
    "from vidar.utils.viz import viz_depth\n",
    "\n",
    "height, width = (384, 640)\n",
    "height_pano, width_pano = (256, 2048)\n",
    "params = {\n",
    "    'name': 'PanoCamOuroboros',\n",
    "    'path': '/data/datasets/DDAD/ddad_train_val/ddad_overfit_000071.json',\n",
    "    # 'path': '/data/datasets/DDAD/ddad_train_val/ddad.json',\n",
    "    'split': 'train',\n",
    "    'context': [-1, 1],\n",
    "    'labels': ['depth', 'pose'],\n",
    "    'cameras': [1, 5, 6, 7, 8, 9],\n",
    "    'depth_type': 'lidar',\n",
    "    'repeat': 1,\n",
    "    'pano_cam_config': Config(**{\n",
    "        'name': 'panocam_150_z_-02_+02',\n",
    "        'height': height_pano,\n",
    "        'width': width_pano,\n",
    "        'position_in_world': [0, 0, 1.5],\n",
    "        'phi_range': [0, 6.2831853072],\n",
    "        'rho': 1.0,\n",
    "        'z_range': [-0.2, 0.2]}),\n",
    "    'data_transform':\n",
    "        get_transforms('train', Config(**{'resize': [height, width]})),\n",
    "}\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = PanoCamOuroborosDataset(**params)\n",
    "print('# of frames: {}'.format(len(dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get a sample\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m scene, \u001b[38;5;241m*\u001b[39m_, timestamp \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcamera_01\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscene\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get a sample\n",
    "data = dataset[0]\n",
    "\n",
    "scene, *_, timestamp = data['camera_01']['filename'][0].split('/')\n",
    "filename = f'{scene}_{timestamp}'\n",
    "panodepth = data['camera_pano']['depth']\n",
    "panodepth_viz = (viz_depth(panodepth[0][0], filter_zeros=False) * 255.0).astype(np.uint8)\n",
    "tensor_to_rgb_viz = lambda x: (x.permute(1, 2, 0) * 255.0).numpy().astype(np.uint8)\n",
    "rgbs = np.hstack(\n",
    "    [tensor_to_rgb_viz(data[c]['rgb'][0]) for c in data.keys() if c.startswith('camera_0')]\n",
    ")\n",
    "\n",
    "rgbs = cv2.resize(rgbs, (panodepth_viz.shape[1], panodepth_viz.shape[0]))\n",
    "\n",
    "# From PanoDepth to ImageDepth\n",
    "panodepth_tensor = torch.FloatTensor(panodepth[0])[None]\n",
    "params = PanoCamera.params_from_config(params['pano_cam_config'].dict)\n",
    "camera = PanoCamera(params['K'][None], params['hw'], params['Twc'][None])\n",
    "\n",
    "# xyz_lidar = camera.reconstruct_depth_map(panodepth_tensor, to_world=True)\n",
    "xyz_lidar = camera.reconstruct_depth_map(panodepth_tensor, to_world=False)\n",
    "xyz_lidar = xyz_lidar.view(3, -1).numpy()\n",
    "rgb_lidar = np.zeros_like(xyz_lidar).T\n",
    "for camera in dataset.sensors:\n",
    "    if not camera.startswith('camera'):\n",
    "        continue\n",
    "    print('RGB from {}'.format(data[camera]['filename'][0]))\n",
    "\n",
    "    K = data[camera]['intrinsics'][0].numpy()\n",
    "    # Twc = data[camera]['extrinsics'][0].numpy()\n",
    "    # xyz_camera = Twc[:3, :3] @ xyz_lidar + Twc[:3, 3:]\n",
    "    Tpc = np.linalg.inv(data[camera]['pose_to_pano'][0])\n",
    "    xyz_camera = Tpc[:3, :3] @ xyz_lidar + Tpc[:3, 3:]\n",
    "    ix, iy, iz = K @ xyz_camera\n",
    "    ix, iy = ((ix / iz).astype(np.int16), (iy / iz).astype(np.int16))\n",
    "\n",
    "    proj_on_image = np.logical_and.reduce([\n",
    "        xyz_camera[2] > 0,\n",
    "        ix >= 0, ix < width,\n",
    "        iy >= 0, iy < height,\n",
    "    ])\n",
    "\n",
    "    image = data[camera]['rgb'][0].permute(1, 2, 0).numpy()\n",
    "    rgb_lidar[proj_on_image] = image[iy[proj_on_image], ix[proj_on_image], :]\n",
    "\n",
    "xyz_lidar = xyz_lidar.T\n",
    "rgb_lidar = (rgb_lidar * 255.0).astype(np.uint8)\n",
    "\n",
    "rgb_lidar_image = rgb_lidar.reshape(height_pano, width_pano, 3)\n",
    "\n",
    "Image.fromarray(np.vstack([rgbs, panodepth_viz, rgb_lidar_image])).save(filename + '_frame.png')\n",
    "print('Save to {}'.format(filename + '_frame.png'))\n",
    "\n",
    "# Save to pcd/ply\n",
    "save_to_ply = False\n",
    "if save_to_ply:\n",
    "    # preview on mac\n",
    "    HEADER = (\n",
    "        'ply',\n",
    "        'format ascii 1.0',\n",
    "        'element vertex {}'.format(len(xyz_lidar)),\n",
    "        'property float x',\n",
    "        'property float y',\n",
    "        'property float z',\n",
    "        'property uchar red',\n",
    "        'property uchar green',\n",
    "        'property uchar blue',\n",
    "        'end_header',\n",
    "    )\n",
    "    suffix = '_point_cloud.ply'\n",
    "    def write_func(f, x, y, z, r, g, b):\n",
    "        f.write(('{:.4f} ' * 6 + '\\n').format(x, y, z, r, g, b))\n",
    "else:\n",
    "    # open3d\n",
    "    HEADER = (\n",
    "        '# .PCD v0.7 - Point Cloud Data file format',\n",
    "        'VERSION 0.7',\n",
    "        'FIELDS x y z rgb',\n",
    "        'SIZE 4 4 4 4',\n",
    "        'TYPE F F F F',\n",
    "        'COUNT 1 1 1 1',\n",
    "        'WIDTH {}'.format(len(xyz_lidar)),\n",
    "        'HEIGHT 1',\n",
    "        'VIEWPOINT 0 0 0 1 0 0 0',\n",
    "        'POINTS {}'.format(len(xyz_lidar)),\n",
    "        'DATA ascii',\n",
    "    )\n",
    "    suffix = '_point_cloud.pcd'\n",
    "    def write_func(f, x, y, z, r, g, b):\n",
    "        rgb = b | g << 8 | r << 16\n",
    "        rgb = unpack('f', pack('i', rgb))[0]\n",
    "        f.write(('{} ' * 3 + '{}\\n').format(x, y, z, rgb))\n",
    "\n",
    "rgb_lidar = rgb_lidar.astype(np.uint32)\n",
    "print('Save to {}'.format(filename + suffix))\n",
    "with open(filename + suffix, 'w') as f:\n",
    "    f.write('\\n'.join(HEADER) + '\\n')\n",
    "    for (x, y, z), (r, g, b) in zip(xyz_lidar, rgb_lidar):\n",
    "        write_func(f, x, y, z, r, g, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
